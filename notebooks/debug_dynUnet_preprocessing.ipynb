{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug data reader and transforms for dynUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import yaml\n",
    "import datetime\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import ignite\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import interpolate\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, PersistentDataset, Dataset\n",
    "from monai.utils import misc\n",
    "from monai.engines import SupervisedTrainer\n",
    "from monai.losses import DiceLoss\n",
    "from monai.networks.nets import DynUNet\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadNiftid,\n",
    "    AddChanneld,\n",
    "    CropForegroundd,\n",
    "    Spacingd,\n",
    "    Orientationd,\n",
    "    SpatialPadd,\n",
    "    NormalizeIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandZoomd,\n",
    "    CastToTyped,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    RandScaleIntensityd,\n",
    "    RandRotated,\n",
    "    RandFlipd,\n",
    "    SqueezeDimd,\n",
    "    ToTensord,\n",
    "    Activationsd,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "sys.path.append(\"/mnt/data/mranzini/Desktop/GIFT-Surg/FBS_Monai/basic_unet_monai/src/\")\n",
    "from io_utils import create_data_list\n",
    "from custom_transform import ConverToOneHotd\n",
    "\n",
    "# print MONAI config information\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print_config()\n",
    "\n",
    "cuda_device=1\n",
    "torch.cuda.set_device(cuda_device)\n",
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and validation lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list folders to search for the data\n",
    "data_root = [\"/mnt/data/mranzini/Desktop/GIFT-Surg/Data/NeuroImage_dataset/GroupA\", \n",
    "             \"/mnt/data/mranzini/Desktop/GIFT-Surg/Data/NeuroImage_dataset/GroupB1\",\n",
    "             \"/mnt/data/mranzini/Desktop/GIFT-Surg/Data/NeuroImage_dataset/GroupB2\", \n",
    "             \"/mnt/data/mranzini/Desktop/GIFT-Surg/Data/NeuroImage_dataset_extension/GroupC\",\n",
    "             \"/mnt/data/mranzini/Desktop/GIFT-Surg/Data/NeuroImage_dataset_extension/GroupD\",\n",
    "             \"/mnt/data/mranzini/Desktop/GIFT-Surg/Data/NeuroImage_dataset_extension/GroupE\",\n",
    "             \"/mnt/data/mranzini/Desktop/GIFT-Surg/Data/NeuroImage_dataset_extension/GroupF\"]\n",
    "\n",
    "# list of subject IDs to search for data\n",
    "list_root = \"/mnt/data/mranzini/Desktop/GIFT-Surg/Retraining_with_expanded_dataset/config/file_names\"\n",
    "training_list = os.path.join(list_root, \"list_train_files.txt\")\n",
    "validation_list = [os.path.join(list_root, \"list_validation_h_files.txt\"),\n",
    "                   os.path.join(list_root, \"list_validation_p_files.txt\")]\n",
    "\n",
    "train_files = create_data_list(data_folder_list=data_root, \n",
    "                               subject_list=training_list, \n",
    "                               img_postfix='_Image', \n",
    "                               label_postfix='_Label')\n",
    "\n",
    "print(len(train_files))\n",
    "print(train_files[0])\n",
    "print(train_files[-1])\n",
    "\n",
    "val_files = create_data_list(data_folder_list=data_root, \n",
    "                             subject_list=validation_list, \n",
    "                             img_postfix='_Image', \n",
    "                             label_postfix='_Label')\n",
    "print(len(val_files))\n",
    "print(val_files[0])\n",
    "print(val_files[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a custom transform to change spacing in-plane only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Any, Dict, Hashable, Mapping, Optional, Sequence, Tuple, Union\n",
    "\n",
    "from monai.transforms import MapTransform, Spacing, Spacingd\n",
    "from monai.config import KeysCollection\n",
    "from monai.utils import (\n",
    "    GridSampleMode,\n",
    "    GridSamplePadMode,\n",
    "    InterpolateMode,\n",
    "    ensure_tuple,\n",
    "    ensure_tuple_rep,\n",
    "    fall_back_tuple,\n",
    ")\n",
    "GridSampleModeSequence = Union[Sequence[Union[GridSampleMode, str]], GridSampleMode, str]\n",
    "GridSamplePadModeSequence = Union[Sequence[Union[GridSamplePadMode, str]], GridSamplePadMode, str]\n",
    "InterpolateModeSequence = Union[Sequence[Union[InterpolateMode, str]], InterpolateMode, str]\n",
    "\n",
    "class InPlaneSpacingd(Spacingd):\n",
    "    def __init__(self, \n",
    "                 keys: KeysCollection,\n",
    "                 pixdim: Sequence[float],\n",
    "                 diagonal: bool = False,\n",
    "                 mode: GridSampleModeSequence = GridSampleMode.BILINEAR,\n",
    "                 padding_mode: GridSamplePadModeSequence = GridSamplePadMode.BORDER,\n",
    "                 align_corners: Union[Sequence[bool], bool] = False,\n",
    "                 dtype: Optional[Union[Sequence[np.dtype], np.dtype]] = np.float64,\n",
    "                 meta_key_postfix: str = \"meta_dict\", \n",
    "        ) -> None:\n",
    "        super().__init__(keys, \n",
    "                         pixdim,\n",
    "                         diagonal,\n",
    "                         mode,\n",
    "                         padding_mode,\n",
    "                         align_corners,\n",
    "                         dtype,\n",
    "                         meta_key_postfix)\n",
    "        self.pixdim = np.array(ensure_tuple(pixdim), dtype=np.float64)\n",
    "        self.diagonal = diagonal\n",
    "        self.dim_to_keep = np.argwhere(self.pixdim == -1.0)\n",
    "\n",
    "    def __call__(self, \n",
    "                data: Mapping[Union[Hashable, str], Dict[str, np.ndarray]]\n",
    "                )-> Dict[Union[Hashable, str], Union[np.ndarray, Dict[str, np.ndarray]]]:\n",
    "        d = dict(data)\n",
    "        for idx, key in enumerate(self.keys):\n",
    "            meta_data = d[f\"{key}_{self.meta_key_postfix}\"]\n",
    "            # set pixdim to original pixdim value where required\n",
    "            current_pixdim = copy.deepcopy(self.pixdim)\n",
    "            original_pixdim = meta_data[\"pixdim\"]\n",
    "            old_pixdim = original_pixdim[1:4]\n",
    "            current_pixdim[self.dim_to_keep] = old_pixdim[self.dim_to_keep]\n",
    "            \n",
    "            # apply the transform\n",
    "            spacing_transform = Spacing(current_pixdim, diagonal=self.diagonal)\n",
    "            \n",
    "            # resample array of each corresponding key\n",
    "            # using affine fetched from d[affine_key]\n",
    "            d[key], _, new_affine = spacing_transform(\n",
    "                data_array=d[key],\n",
    "                affine=meta_data[\"affine\"],\n",
    "                mode=self.mode[idx],\n",
    "                padding_mode=self.padding_mode[idx],\n",
    "                align_corners=self.align_corners[idx],\n",
    "                dtype=self.dtype[idx],\n",
    "            )\n",
    "            \n",
    "            # store the modified affine\n",
    "            meta_data[\"affine\"] = new_affine\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_labels = [0, 1]\n",
    "pixdim = [0.8, 0.8, -1.0]\n",
    "transf1 = Compose([\n",
    "    LoadNiftid(keys=[\"image\", \"label\"]),\n",
    "    ConverToOneHotd(keys=[\"label\"], labels=seg_labels),\n",
    "    AddChanneld(keys=[\"image\"]),\n",
    "])\n",
    "\n",
    "transf2 = Compose([transf1,\n",
    "                  InPlaneSpacingd(keys=[\"image\", \"label\"], \n",
    "                    pixdim=pixdim,\n",
    "                    mode=[\"bilinear\", \"nearest\"])])\n",
    "\n",
    "\n",
    "data = transf1(train_files[1])\n",
    "print(data[\"image_meta_dict\"][\"pixdim\"])\n",
    "print(data[\"image_meta_dict\"][\"original_affine\"])\n",
    "print(data[\"image_meta_dict\"][\"affine\"])\n",
    "print(data[\"image\"].shape, data[\"label\"].shape)\n",
    "\n",
    "data2 = transf2(train_files[1])\n",
    "print(data2[\"image_meta_dict\"][\"pixdim\"])\n",
    "print(data2[\"image_meta_dict\"][\"original_affine\"])\n",
    "print(data2[\"image_meta_dict\"][\"affine\"])\n",
    "print(data2[\"image\"].shape, data2[\"label\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_z = 30\n",
    "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(12, 10))\n",
    "\n",
    "ax[0, 0].imshow(np.squeeze(data[\"image\"][0, :, :, slice_z]), cmap=\"gray\", interpolation=None)\n",
    "ax[0, 1].imshow(np.squeeze(data[\"label\"][0, :, :, slice_z]), vmin=0.0, vmax=1.0, interpolation=None)\n",
    "ax[0, 2].imshow(np.squeeze(data[\"label\"][1, :, :, slice_z]), vmin=0.0, vmax=1.0, interpolation=None)\n",
    "ax[1, 0].imshow(np.squeeze(data2[\"image\"][0, :, :, slice_z]), cmap=\"gray\", interpolation=None)\n",
    "ax[1, 1].imshow(np.squeeze(data2[\"label\"][0, :, :, slice_z]), vmin=0.0, vmax=1.0, interpolation=None)\n",
    "ax[1, 2].imshow(np.squeeze(data2[\"label\"][1, :, :, slice_z]), vmin=0.0, vmax=1.0, interpolation=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test full series of transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_transform import ConverToOneHotd, InPlaneSpacingd\n",
    "\n",
    "spacing = (0.8, 0.8, -1.0)\n",
    "seg_labels = [0, 1]\n",
    "patch_size = (448, 512, 1)\n",
    "num_classes = len(seg_labels)\n",
    "batch_size = 4\n",
    "\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            LoadNiftid(keys=[\"image\", \"label\"]),\n",
    "            ConverToOneHotd(keys=[\"label\"], labels=seg_labels),\n",
    "            AddChanneld(keys=[\"image\"]),\n",
    "            CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "            InPlaneSpacingd(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                pixdim=spacing,\n",
    "                mode=(\"bilinear\", \"nearest\"),\n",
    "            ),\n",
    "            SpatialPadd(keys=[\"image\", \"label\"], spatial_size=patch_size,\n",
    "                       mode=[\"constant\", \"edge\"]),\n",
    "            NormalizeIntensityd(keys=[\"image\"], nonzero=False, channel_wise=True),\n",
    "            RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=patch_size, random_size=False),\n",
    "            SqueezeDimd(keys=[\"image\", \"label\"], dim=-1),\n",
    "            RandZoomd(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                min_zoom=0.9,\n",
    "                max_zoom=1.2,\n",
    "                mode=(\"bilinear\", \"nearest\"),\n",
    "                align_corners=(True, None),\n",
    "                prob=0.16,\n",
    "            ),\n",
    "            CastToTyped(keys=[\"image\", \"label\"], dtype=(np.float32, np.uint8)),\n",
    "            RandGaussianNoised(keys=[\"image\"], std=0.01, prob=0.15),\n",
    "            RandGaussianSmoothd(\n",
    "                keys=[\"image\"],\n",
    "                sigma_x=(0.5, 1.15),\n",
    "                sigma_y=(0.5, 1.15),\n",
    "                sigma_z=(0.5, 1.15),\n",
    "                prob=0.15,\n",
    "            ),\n",
    "            RandScaleIntensityd(keys=[\"image\"], factors=0.3, prob=0.15),\n",
    "            RandRotated(keys=[\"image\", \"label\"], range_x=90, range_y=90, prob=0.2,\n",
    "                        keep_size=True, mode=[\"bilinear\", \"nearest\"],\n",
    "                        padding_mode=[\"zeros\", \"border\"]),\n",
    "            RandFlipd([\"image\", \"label\"], spatial_axis=[0, 1], prob=0.5),   \n",
    "            ToTensord(keys=[\"image\", \"label\"]),\n",
    "        ]\n",
    "    )\n",
    "# check on single data point\n",
    "check_transforms = train_transforms(train_files[10])\n",
    "print(\"Before data loader:\")\n",
    "print(check_transforms[\"image\"].shape, check_transforms[\"label\"].shape)\n",
    "\n",
    "# create data loader and check correctness\n",
    "train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1)\n",
    "check_data = misc.first(train_loader)\n",
    "print(\"Training data tensor shapes\")\n",
    "print(check_data[\"image\"].shape, check_data[\"label\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_element in range(batch_size): \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.title(f\"Batch element = {batch_element}\")\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(check_data[\"image\"][batch_element, 0, :, :], cmap='gray')\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(check_data[\"label\"][batch_element, 0, :, :], interpolation=\"nearest\", vmin=0.0, vmax=1.0)\n",
    "    print(\"Segmentation limits: channel 0\")\n",
    "    print(torch.min(check_data[\"label\"][batch_element, 0, :, :]))\n",
    "    print(torch.max(check_data[\"label\"][batch_element, 0, :, :]))\n",
    "    if num_classes == 2:\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(check_data[\"label\"][batch_element, 1, : , :], interpolation=\"nearest\", vmin=0.0, vmax=1.0)\n",
    "        print(\"Segmentation limits: channel 1\")\n",
    "        print(torch.min(check_data[\"label\"][batch_element, 1, :, :]))\n",
    "        print(torch.max(check_data[\"label\"][batch_element, 1, :, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai0.3.0",
   "language": "python",
   "name": "monai0.3.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
