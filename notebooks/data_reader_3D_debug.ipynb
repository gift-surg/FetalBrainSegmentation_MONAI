{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data reading and preprocessing for fetal brain segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from glob import glob\n",
    "import logging\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator, _prepare_batch\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import monai\n",
    "from monai.data import NiftiDataset, list_data_collate\n",
    "from monai.transforms import (\n",
    "    Activationsd,\n",
    "    AddChanneld,\n",
    "    NormalizeIntensityd,\n",
    "    AsDiscreted,\n",
    "    Resized,\n",
    "    Compose,\n",
    "    KeepLargestConnectedComponentd,\n",
    "    LoadNiftid,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandRotated,\n",
    "    RandFlipd,\n",
    "    ToTensord,\n",
    "    MapTransform,\n",
    "    CropForegroundd,\n",
    "    SpatialCrop\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "\n",
    "# from ipynb.fs.full.io_utils import create_data_list\n",
    "sys.path.append(\"/mnt/data/mranzini/Desktop/GIFT-Surg/FBS_Monai/basic_unet_monai/src/\")\n",
    "from io_utils import create_data_list\n",
    "from custom_transform import ConverToOneHotd, MinimumPadd, CropForegroundAnisotropicMargind\n",
    "\n",
    "monai.config.print_config()\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "cuda_device=1\n",
    "torch.cuda.set_device(cuda_device)\n",
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and validation data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list folders to search for the data\n",
    "data_root = [\"/mnt/data/mranzini/Desktop/GIFT-Surg/Data/NeuroImage_dataset/GroupA\", \n",
    "             \"/mnt/data/mranzini/Desktop/GIFT-Surg/Data/NeuroImage_dataset/GroupB1\",\n",
    "             \"/mnt/data/mranzini/Desktop/GIFT-Surg/Data/NeuroImage_dataset/GroupB2\", \n",
    "             \"/mnt/data/mranzini/Desktop/GIFT-Surg/Data/NeuroImage_dataset_extension/GroupC\",\n",
    "             \"/mnt/data/mranzini/Desktop/GIFT-Surg/Data/NeuroImage_dataset_extension/GroupD\",\n",
    "             \"/mnt/data/mranzini/Desktop/GIFT-Surg/Data/NeuroImage_dataset_extension/GroupE\",\n",
    "             \"/mnt/data/mranzini/Desktop/GIFT-Surg/Data/NeuroImage_dataset_extension/GroupF\"]\n",
    "\n",
    "# list of subject IDs to search for data\n",
    "list_root = \"/mnt/data/mranzini/Desktop/GIFT-Surg/Retraining_with_expanded_dataset/config/file_names\"\n",
    "training_list = os.path.join(list_root, \"list_train_files.txt\")\n",
    "validation_list = [os.path.join(list_root, \"list_validation_h_files.txt\"),\n",
    "                   os.path.join(list_root, \"list_validation_p_files.txt\")]\n",
    "\n",
    "# \n",
    "train_files = create_data_list(data_folder_list=data_root, \n",
    "                               subject_list=training_list, \n",
    "                               img_postfix='_Image', \n",
    "                               label_postfix='_Label')\n",
    "\n",
    "print(len(train_files))\n",
    "print(train_files[0])\n",
    "print(train_files[-1])\n",
    "\n",
    "val_files = create_data_list(data_folder_list=data_root, \n",
    "                             subject_list=validation_list, \n",
    "                             img_postfix='_Image', \n",
    "                             label_postfix='_Label')\n",
    "print(len(val_files))\n",
    "print(val_files[0])\n",
    "print(val_files[-1])\n",
    "\n",
    "# np.savetxt(\"images_training.txt\", images_training, fmt='%s')\n",
    "# np.savetxt(\"seg_training.txt\", seg_training, fmt='%s')\n",
    "# np.savetxt(\"images_validation.txt\", images_validation, fmt='%s')\n",
    "# np.savetxt(\"seg_validation.txt\", seg_validation, fmt='%s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup transforms, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "seg_labels = [1]\n",
    "patch_size = [96, 96, 36] \n",
    "\n",
    "# data preprocessing for training:\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadNiftid(keys=[\"img\", \"seg\"]),\n",
    "        ConverToOneHotd(keys=[\"seg\"], labels=seg_labels),\n",
    "        AddChanneld(keys=[\"img\"]),\n",
    "        NormalizeIntensityd(keys=[\"img\"]),\n",
    "        MinimumPadd(keys=[\"img\", \"seg\"], k=(-1, -1, patch_size[2])),\n",
    "        Resized(keys=[\"img\", \"seg\"], spatial_size=(patch_size[0], patch_size[1], -1)),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"img\", \"seg\"], label_key=\"seg\", spatial_size=patch_size, pos=1, neg=1, num_samples=2\n",
    "        ),\n",
    "#         RandRotated(keys=[\"img\", \"seg\"], range_x=90, range_y=90, prob=0.5, keep_size=True,\n",
    "#                     mode=[\"bilinear\", \"nearest\"]),\n",
    "#         RandFlipd(keys=[\"img\", \"seg\"], spatial_axis=[0, 1]),\n",
    "        ToTensord(keys=[\"img\", \"seg\"]),\n",
    "    ]\n",
    ")\n",
    "# create training data loader\n",
    "check_train_files = train_files\n",
    "print(len(check_train_files))\n",
    "\n",
    "# define dataset, data loader\n",
    "check_ds = monai.data.Dataset(data=check_train_files[:4], transform=train_transforms)\n",
    "# use batch_size=2 to load images \n",
    "check_loader = monai.data.DataLoader(check_ds,\n",
    "                                     batch_size=1,\n",
    "                                     shuffle=True, num_workers=1,\n",
    "                                     pin_memory=torch.cuda.is_available())\n",
    "\n",
    "check_data = monai.utils.misc.first(check_loader)\n",
    "print(check_data[\"img\"].shape, check_data[\"seg\"].shape)\n",
    "check_data = monai.utils.misc.first(check_loader)\n",
    "print(check_data[\"img\"].shape, check_data[\"seg\"].shape)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(131)\n",
    "plt.imshow(check_data['img'][0, 0, :, :, 8], cmap='gray')\n",
    "plt.subplot(132)\n",
    "plt.imshow(check_data['seg'][0, 0, :, :, 8], interpolation=\"nearest\")\n",
    "print(\"Segmentation limits: channel 0\")\n",
    "print(torch.min(check_data['seg'][0, 0, :, :, 8]))\n",
    "print(torch.max(check_data['seg'][0, 0, :, :, 8]))\n",
    "if num_classes == 2:\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(check_data['seg'][0, 1, : , :, 8], interpolation=\"nearest\")\n",
    "    print(\"Segmentation limits: channel 1\")\n",
    "    print(torch.min(check_data['seg'][0, 1, :, :, 8]))\n",
    "    print(torch.max(check_data['seg'][0, 1, :, :, 8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms using the segmentation as bounding box for cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, Hashable, List, Mapping, Optional, Sequence, Tuple, Union\n",
    "from monai.utils import Method, NumpyPadMode, ensure_tuple, ensure_tuple_rep, fall_back_tuple\n",
    "\n",
    "from monai.config import IndexSelection, KeysCollection\n",
    "\n",
    "def generate_spatial_bounding_box_anisotropic_margin(\n",
    "    img: np.ndarray,\n",
    "    select_fn: Callable = lambda x: x > 0,\n",
    "    channel_indices: Optional[IndexSelection] = None,\n",
    "    margin: Optional[Sequence[int]] = 0,\n",
    ") -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    generate the spatial bounding box of foreground in the image with start-end positions.\n",
    "    Users can define arbitrary function to select expected foreground from the whole image or specified channels.\n",
    "    And it can also add margin to every dim of the bounding box.\n",
    "    Args:\n",
    "        img: source image to generate bounding box from.\n",
    "        select_fn: function to select expected foreground, default is to select values > 0.\n",
    "        channel_indices: if defined, select foreground only on the specified channels\n",
    "            of image. if None, select foreground on the whole image.\n",
    "        margin: add margin to all dims of the bounding box.\n",
    "    \"\"\"\n",
    "#     assert isinstance(margin, int), \"margin must be int type.\"\n",
    "    data = img[[*(ensure_tuple(channel_indices))]] if channel_indices is not None else img\n",
    "    data = np.any(select_fn(data), axis=0)\n",
    "    nonzero_idx = np.nonzero(data)\n",
    "    \n",
    "    if isinstance(margin, int):\n",
    "        margin = ensure_tuple_rep(margin, len(data.shape))\n",
    "    margin = [m if m > 0 else 0 for m in margin]\n",
    "    assert len(data.shape) == len(margin), \"defined margin has different number of dimensions than input\"\n",
    "\n",
    "    box_start = list()\n",
    "    box_end = list()\n",
    "    for i in range(data.ndim):\n",
    "        assert len(nonzero_idx[i]) > 0, f\"did not find nonzero index at spatial dim {i}\"\n",
    "        box_start.append(max(0, np.min(nonzero_idx[i]) - margin[i]))\n",
    "        box_end.append(min(data.shape[i], np.max(nonzero_idx[i]) + margin[i] + 1))\n",
    "    return box_start, box_end\n",
    "\n",
    "class MyCropForegroundd(MapTransform):\n",
    "    \"\"\"\n",
    "    Dictionary-based version :py:class:`monai.transforms.CropForeground`.\n",
    "    Crop only the foreground object of the expected images.\n",
    "    The typical usage is to help training and evaluation if the valid part is small in the whole medical image.\n",
    "    The valid part can be determined by any field in the data with `source_key`, for example:\n",
    "    - Select values > 0 in image field as the foreground and crop on all fields specified by `keys`.\n",
    "    - Select label = 3 in label field as the foreground to crop on all fields specified by `keys`.\n",
    "    - Select label > 0 in the third channel of a One-Hot label field as the foreground to crop all `keys` fields.\n",
    "    Users can define arbitrary function to select expected foreground from the whole source image or specified\n",
    "    channels. And it can also add margin to every dim of the bounding box of foreground object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        source_key: str,\n",
    "        select_fn: Callable = lambda x: x > 0,\n",
    "        channel_indices: Optional[IndexSelection] = None,\n",
    "        margin: Optional[Sequence[int]] = 0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            keys: keys of the corresponding items to be transformed.\n",
    "                See also: :py:class:`monai.transforms.compose.MapTransform`\n",
    "            source_key: data source to generate the bounding box of foreground, can be image or label, etc.\n",
    "            select_fn: function to select expected foreground, default is to select values > 0.\n",
    "            channel_indices: if defined, select foreground only on the specified channels\n",
    "                of image. if None, select foreground on the whole image.\n",
    "            margin: add margin to dims of the bounding box.\n",
    "        \"\"\"\n",
    "        super().__init__(keys)\n",
    "        self.source_key = source_key\n",
    "        self.select_fn = select_fn\n",
    "        self.channel_indices = ensure_tuple(channel_indices) if channel_indices is not None else None\n",
    "        self.margin = margin\n",
    "\n",
    "    def __call__(self, data: Mapping[Hashable, np.ndarray]) -> Dict[Hashable, np.ndarray]:\n",
    "        d = dict(data)\n",
    "        box_start, box_end = generate_spatial_bounding_box_anisotropic_margin(\n",
    "            d[self.source_key], self.select_fn, self.channel_indices, self.margin\n",
    "        )\n",
    "        cropper = SpatialCrop(roi_start=box_start, roi_end=box_end)\n",
    "        for key in self.keys:\n",
    "            d[key] = cropper(d[key])\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test another set of transforms to use the segmentation to crop a bounding box the foreground label\n",
    "num_classes = 1\n",
    "seg_labels = [1]\n",
    "patch_size = [96, 96, 36] \n",
    "\n",
    "# data preprocessing for training:\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadNiftid(keys=[\"img\", \"seg\"]),\n",
    "        ConverToOneHotd(keys=[\"seg\"], labels=seg_labels),\n",
    "        AddChanneld(keys=[\"img\"]),\n",
    "        NormalizeIntensityd(keys=[\"img\"]),\n",
    "        CropForegroundAnisotropicMargind(keys=[\"img\", \"seg\"], source_key=\"seg\", margin=[20, 20, 5]),\n",
    "        MinimumPadd(keys=[\"img\", \"seg\"], k=(-1, -1, patch_size[2])),\n",
    "        Resized(keys=[\"img\", \"seg\"], spatial_size=(patch_size[0], patch_size[1], -1), mode=[\"trilinear\", \"nearest\"]),\n",
    "        RandRotated(keys=[\"img\", \"seg\"], range_x=90, range_y=90, prob=0.5, keep_size=True,\n",
    "                    mode=[\"bilinear\", \"nearest\"]),\n",
    "        RandFlipd(keys=[\"img\", \"seg\"], spatial_axis=[0, 1]),\n",
    "        ToTensord(keys=[\"img\", \"seg\"]),\n",
    "    ]\n",
    ")\n",
    "# create training data loader\n",
    "check_train_files = train_files\n",
    "print(len(check_train_files))\n",
    "\n",
    "# define dataset, data loader\n",
    "check_ds = monai.data.Dataset(data=check_train_files[:4], transform=train_transforms)\n",
    "# use batch_size=2 to load images \n",
    "check_loader = monai.data.DataLoader(check_ds,\n",
    "                                     batch_size=1,\n",
    "                                     shuffle=True, num_workers=1,\n",
    "                                     pin_memory=torch.cuda.is_available())\n",
    "\n",
    "check_data = monai.utils.misc.first(check_loader)\n",
    "print(check_data[\"img\"].shape, check_data[\"seg\"].shape)\n",
    "check_data = monai.utils.misc.first(check_loader)\n",
    "print(check_data[\"img\"].shape, check_data[\"seg\"].shape)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(131)\n",
    "plt.imshow(check_data['img'][0, 0, :, :, 18], cmap='gray')\n",
    "plt.subplot(132)\n",
    "plt.imshow(check_data['seg'][0, 0, :, :, 18], interpolation=\"nearest\")\n",
    "print(\"Segmentation limits: channel 0\")\n",
    "print(torch.min(check_data['seg'][0, 0, :, :, 18]))\n",
    "print(torch.max(check_data['seg'][0, 0, :, :, 18]))\n",
    "if num_classes == 2:\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(check_data['seg'][0, 1, : , :, 18], interpolation=\"nearest\")\n",
    "    print(\"Segmentation limits: channel 1\")\n",
    "    print(torch.min(check_data['seg'][0, 1, :, :, 18]))\n",
    "    print(torch.max(check_data['seg'][0, 1, :, :, 18]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(check_data['seg'][0, 0, 70, 10:40, 18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai0.2.0-venv",
   "language": "python",
   "name": "monai0.2.0-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
